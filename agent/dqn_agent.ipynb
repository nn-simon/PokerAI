{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 0]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/envs/poker/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(SUIT_TO_INT_ENC.transform(_explode_array(['H', 'S', 'D', 'C'])))\n",
    "a = SUIT_TO_INT_ENC.transform(_explode_array(['H', 'S', 'D', 'C']))\n",
    "print(SUIT_INT_TO_ONEHOT_ENC.transform(_explode_array(a)))\n",
    "VALUE_INT_TO_ONEHOT_ENC.transform(_explode_array([3, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/envs/poker/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/simon/anaconda3/envs/poker/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# %load dqn_agent.py\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "def _explode_array(array):\n",
    "    return [[a] for a in array]\n",
    "\n",
    "SUIT_TO_INT_ENC = LabelEncoder().fit(['H', 'S', 'D', 'C'])\n",
    "SUIT_INT_TO_ONEHOT_ENC = OneHotEncoder(sparse=False).fit(_explode_array(range(0, 4)))\n",
    "VALUE_INT_TO_ONEHOT_ENC = OneHotEncoder(sparse=False).fit(_explode_array(range(2, 15)))\n",
    "MEMORY = deque()\n",
    "\n",
    "def suits_to_onehot(suits):\n",
    "    def _suits_to_ints(suits):\n",
    "        return SUIT_TO_INT_ENC.transform(_explode_array(suits))\n",
    "\n",
    "    def _suit_ints_to_onehot(suits):\n",
    "        return SUIT_INT_TO_ONEHOT_ENC.transform(_explode_array(suits))\n",
    "\n",
    "    return _suit_ints_to_onehot(_suits_to_ints(suits))\n",
    "\n",
    "def card_values_to_onehot(values):\n",
    "    return VALUE_INT_TO_ONEHOT_ENC.transform(_explode_array(values))\n",
    "\n",
    "def clear_memory():\n",
    "    MEMORY.clear()\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, num_agents, starting_epsilon, e_min, e_decay, gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.epsilon = starting_epsilon  # exploration rate\n",
    "        self.epsilon_min = e_min\n",
    "        self.epsilon_decay = e_decay\n",
    "        self.gamma = gamma  # discount rate\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(64, input_shape=(1, self.state_size), activation='relu', return_sequences=True))\n",
    "        model.add(SimpleRNN(32, activation='relu'))\n",
    "        # model.add(Dense(64, activation='relu'))\n",
    "        # model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',       # if you change this, make sure to change it in set_model\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        MEMORY.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape((1, 1, len(state)))\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.uniform(low=-10, high=10, size=(self.action_size,))\n",
    "\n",
    "        act_values = self.model.predict([state])[0]\n",
    "        return act_values  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if batch_size > len(MEMORY):\n",
    "            return\n",
    "        minibatch = random.sample(MEMORY, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if state is None:\n",
    "                continue\n",
    "            state = state.reshape((1,1,len(state)))\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = next_state.reshape((1,1,len(next_state)))\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def clear_session(self):\n",
    "        K.clear_session()\n",
    "\n",
    "    def make_features(self, valid_actions, hole_cards, game_state):\n",
    "        player_idx = game_state['next_player']\n",
    "        player_uuid = game_state['seats'][player_idx]['uuid']\n",
    "        bb_amount = game_state['small_blind_amount'] * 2\n",
    "\n",
    "        # split hole cards, onehot suits and values\n",
    "\n",
    "        hole_suits, hole_values = self._cards_to_arrays(hole_cards)\n",
    "        hole_suits = suits_to_onehot(hole_suits)\n",
    "        hole_values = card_values_to_onehot(hole_values)\n",
    "\n",
    "        # river cards\n",
    "        temp_suit_zeros = np.zeros((5, 4))\n",
    "        temp_value_zeros = np.zeros((5, 13))\n",
    "        river_suits, river_values = self._cards_to_arrays(game_state['community_card'])\n",
    "        if river_suits and river_values:\n",
    "            river_suits = suits_to_onehot(river_suits)\n",
    "            river_values = card_values_to_onehot(river_values)\n",
    "            temp_suit_zeros[:river_suits.shape[0], :river_suits.shape[1]] = river_suits # 0-padding\n",
    "            temp_value_zeros[:river_values.shape[0], :river_values.shape[1]] = river_values\n",
    "\n",
    "        river_suits = temp_suit_zeros\n",
    "        river_values = temp_value_zeros\n",
    "\n",
    "        # pot\n",
    "        total_main_amount = game_state['pot']['main']['amount']\n",
    "        total_side_pot = sum([a['amount'] for a in game_state['pot']['side']])\n",
    "        total_pot_as_bb = [(total_main_amount + total_side_pot) / bb_amount]\n",
    "\n",
    "        # own stack size\n",
    "        own_stack_size = [game_state['seats'][player_idx]['stack'] / bb_amount]\n",
    "\n",
    "        # other players stack size\n",
    "        players_after_stacks = [p['stack'] / bb_amount for p in game_state['seats'][player_idx + 1:]]\n",
    "        players_before_stacks = [p['stack'] / bb_amount for p in game_state['seats'][:player_idx]]\n",
    "        players_after_stacks.extend(players_before_stacks)\n",
    "        other_players_stack_sizes = players_after_stacks\n",
    "\n",
    "        # TODO: distance from button, scaled from 0 to 1\n",
    "        # n_players = len(game_state['seats'])\n",
    "        # distance = ()\n",
    "\n",
    "        # players folded? (binary)\n",
    "        players_after_folds = [int(p['state'] == 'folded') for p in game_state['seats'][player_idx + 1:]]\n",
    "        players_before_folds = [int(p['state'] == 'folded') for p in game_state['seats'][:player_idx]]\n",
    "        players_after_folds.extend(players_before_folds)\n",
    "        player_folds = players_after_folds\n",
    "\n",
    "        # action history, for use below\n",
    "        game_state_histories = (game_state['action_histories'].values())\n",
    "        action_history = [action for phase in game_state_histories for action in phase]\n",
    "\n",
    "        # money put into pot by each player since our last\n",
    "        moves_since_our_last = []\n",
    "        for action in action_history[::-1]:\n",
    "            if action['uuid'] != player_uuid:\n",
    "                moves_since_our_last.append(action)\n",
    "            else:\n",
    "                break\n",
    "        moves_since_our_last.reverse()\n",
    "        moves_since_our_last = moves_since_our_last[:self.num_agents]\n",
    "        temp_move_zeroes = np.zeros(self.num_agents)\n",
    "        money_since_our_last_move = [a.get('amount', 0) for a in moves_since_our_last]\n",
    "        for i, m in enumerate(money_since_our_last_move):\n",
    "            temp_move_zeroes[i] = m\n",
    "        money_since_our_last_move = temp_move_zeroes\n",
    "\n",
    "        # amt to call\n",
    "        amt_to_call = [0]\n",
    "        for action in valid_actions:\n",
    "            if action['action'] == 'call':\n",
    "                amt_to_call = [action['amount'] / bb_amount]\n",
    "                break\n",
    "\n",
    "        min_raise, max_raise = valid_actions[2]['amount']['min'] / bb_amount, valid_actions[2]['amount']['max'] / bb_amount\n",
    "\n",
    "        feature_arrays = [hole_values, hole_suits, river_values, river_suits, total_pot_as_bb,\n",
    "                own_stack_size, other_players_stack_sizes, player_folds, money_since_our_last_move,\n",
    "                amt_to_call, min_raise, max_raise]\n",
    "\n",
    "        ret = None\n",
    "\n",
    "        for array in feature_arrays:\n",
    "            array = np.array(array).flatten()\n",
    "            if ret is not None:\n",
    "                ret = np.concatenate((ret, array))\n",
    "            else:\n",
    "                ret = array\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def _cards_to_arrays(self, cards):\n",
    "        suits = []\n",
    "        values = []\n",
    "        for card in cards:\n",
    "            if card[1:] == 'A': card = card[0] + '14'\n",
    "            if card[1:] == 'K': card = card[0] + '13'\n",
    "            if card[1:] == 'Q': card = card[0] + '12'\n",
    "            if card[1:] == 'J': card = card[0] + '11'\n",
    "            if card[1:] == 'T': card = card[0] + '10'\n",
    "            suits.append(card[0])\n",
    "            values.append(int(card[1:]))\n",
    "        return suits, values\n",
    "\n",
    "    def set_model(self, model, weights):\n",
    "        self.model = model\n",
    "        self.model.set_weights(weights)\n",
    "        self.model.compile(loss='mse',\n",
    "                           optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "    def get_init_info(self):\n",
    "        '''Return info as array. Easy to use to reinstantiate the agent'''\n",
    "        info = [\n",
    "            self.state_size,\n",
    "            self.action_size,\n",
    "            self.num_agents,\n",
    "            self.epsilon,\n",
    "            self.epsilon_min,\n",
    "            self.epsilon_decay,\n",
    "            self.gamma\n",
    "        ]\n",
    "        return info\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
